{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "from interpret import show\n",
    "from pygam import LogisticGAM, LinearGAM\n",
    "from gaminet import GAMINet\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, RobustScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from load_datasets import load_adult_data, load_crimes_data\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "\n",
    "random_state = 1\n",
    "task = 'classification'  # regression or classification\n",
    "\n",
    "dataset, y_word_dict = load_adult_data()\n",
    "dataset_name = 'adult'\n",
    "\n",
    "X = pd.DataFrame(dataset['full']['X'])\n",
    "y = np.array(dataset['full']['y'])\n",
    "X, y = shuffle(X, y, random_state=random_state)\n",
    "\n",
    "is_cat = np.array([dt.kind == 'O' for dt in X.dtypes])\n",
    "\n",
    "num_cols = X.columns.values[~is_cat]\n",
    "# one hot encoder pipeline replaced by pd.getdummies to make sure column names are concatenated\n",
    "# cat_ohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "\n",
    "# Handle unknown data as ignore\n",
    "X = pd.get_dummies(X)\n",
    "X = X.reindex(columns=X.columns, fill_value=0)\n",
    "dummy_column_names = X.columns\n",
    "\n",
    "# cat_pipe = Pipeline([cat_ohe_step])\n",
    "num_pipe = Pipeline([('identity', FunctionTransformer())])  # , ('scaler', RobustScaler())])\n",
    "transformers = [\n",
    "    # ('cat', cat_pipe, cat_cols) replaced by pd.getdummies\n",
    "    ('num', num_pipe, num_cols)\n",
    "]\n",
    "ct = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "ct.fit(X)\n",
    "X = ct.transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=dummy_column_names)\n",
    "\n",
    "scaler_dict = {}\n",
    "for c in num_cols:\n",
    "\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X[c] = scaler.fit_transform(X[c].values.reshape(-1, 1))\n",
    "    scaler_dict[c] = scaler\n",
    "\n",
    "# scaler = MinMaxScaler((0, 1))\n",
    "    # scaler.fit([[0.0], [1.0]])\n",
    "    # X[c] = scaler.transform(X[c].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def feature_importance_visualize(data_dict_global, folder=\"./results/\", name=\"demo\", save_png=False, save_eps=False):\n",
    "    all_ir = []\n",
    "    all_names = []\n",
    "    for key, item in data_dict_global.items():\n",
    "        if item[\"importance\"] > 0:\n",
    "            all_ir.append(item[\"importance\"])\n",
    "            all_names.append(key)\n",
    "\n",
    "    max_ids = len(all_names)\n",
    "    if max_ids > 0:\n",
    "        fig = plt.figure(figsize=(0.4 + 0.6 * max_ids, 4))\n",
    "        ax = plt.axes()\n",
    "        ax.bar(np.arange(len(all_ir)), [ir for ir, _ in sorted(zip(all_ir, all_names))][::-1])\n",
    "        ax.set_xticks(np.arange(len(all_ir)))\n",
    "        ax.set_xticklabels([name for _, name in sorted(zip(all_ir, all_names))][::-1], rotation=60)\n",
    "        plt.xlabel(\"Feature Name\", fontsize=12)\n",
    "        plt.ylim(0, np.max(all_ir) + 0.05)\n",
    "        plt.xlim(-1, len(all_names))\n",
    "        plt.title(\"Feature Importance\")\n",
    "\n",
    "        save_path = folder + name\n",
    "        if save_eps:\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            fig.savefig(\"%s.eps\" % save_path, bbox_inches=\"tight\", dpi=100)\n",
    "        if save_png:\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            fig.savefig(\"%s.png\" % save_path, bbox_inches=\"tight\", dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def make_plot(x, mean, upper_bounds, lower_bounds, feature_name, model_name, dataset_name, num_epochs='', debug=False):\n",
    "    x = np.array(x)\n",
    "    if debug:\n",
    "        print(\"Num cols:\", num_cols)\n",
    "    if feature_name in num_cols:\n",
    "        if debug:\n",
    "            print(\"Feature to scale back:\", feature_name)\n",
    "        if feature_name == \"capital.gain\":\n",
    "            pass # halt here\n",
    "        x = scaler_dict[feature_name].inverse_transform(x.reshape(-1, 1)).squeeze()\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"Feature not to scale back:\", feature_name)\n",
    "\n",
    "    plt.plot(x, mean, color='black')\n",
    "    plt.fill_between(x, lower_bounds, mean, color='gray')\n",
    "    plt.fill_between(x, mean, upper_bounds, color='gray')\n",
    "    plt.xlabel(f'Feature value')\n",
    "    plt.ylabel('Feature effect on model output')\n",
    "    plt.title(f'Feature:{feature_name}')\n",
    "    plt.savefig(f'plots/{model_name}_{dataset_name}_shape_{feature_name}_{num_epochs}epochs.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# def plot_continuous_bar(\n",
    "#     data_dict, feature_name, model_name, dataset_name, multiclass=False, show_error=True, title=None, xtitle=\"\", ytitle=\"\"\n",
    "# ):\n",
    "#     if feature_name == \"capital.gain\":\n",
    "#         print(\"BUG\")\n",
    "#\n",
    "#     if data_dict.get(\"scores\", None) is None:  # pragma: no cover\n",
    "#         return None\n",
    "#\n",
    "#     x_vals = data_dict[\"names\"].copy()\n",
    "#     y_vals = data_dict[\"scores\"].copy()\n",
    "#     y_hi = data_dict.get(\"upper_bounds\", None)\n",
    "#     y_lo = data_dict.get(\"lower_bounds\", None)\n",
    "#\n",
    "#     # x_min = min(x_vals)\n",
    "#     # x_max = max(x_vals)\n",
    "#\n",
    "#     if y_hi is None or multiclass:\n",
    "#         show_error = False\n",
    "#\n",
    "#     def extend_x_range(x):\n",
    "#         return x\n",
    "#\n",
    "#     def extend_y_range(y):\n",
    "#         return np.r_[y, y[np.newaxis, -1]]\n",
    "#\n",
    "#     new_x_vals = extend_x_range(x_vals)\n",
    "#     new_y_vals = extend_y_range(y_vals)\n",
    "#     if show_error:\n",
    "#         new_y_hi = extend_y_range(y_hi)\n",
    "#         new_y_lo = extend_y_range(y_lo)\n",
    "#\n",
    "#     data = []\n",
    "#     fill = \"none\"\n",
    "#     if show_error:\n",
    "#         fill = \"tonexty\"\n",
    "#\n",
    "#     if multiclass:\n",
    "#         for i in range(y_vals.shape[1]):\n",
    "#             class_name = (\n",
    "#                 \"Class {}\".format(i)\n",
    "#                 if \"meta\" not in data_dict\n",
    "#                 else data_dict[\"meta\"][\"label_names\"][i]\n",
    "#             )\n",
    "#             class_line = go.Scatter(\n",
    "#                 x=new_x_vals,\n",
    "#                 y=new_y_vals[:, i],\n",
    "#                 line=dict(shape=\"hv\"),\n",
    "#                 name=class_name,\n",
    "#                 mode=\"lines\",\n",
    "#             )\n",
    "#             data.append(class_line)\n",
    "#     else:\n",
    "#         main_line = go.Scatter(\n",
    "#             x=new_x_vals,\n",
    "#             y=new_y_vals,\n",
    "#             name=\"Main\",\n",
    "#             mode=\"lines\",\n",
    "#             line=dict(color=\"rgb(31, 119, 180)\", shape=\"hv\"),\n",
    "#             fillcolor=\"rgba(68, 68, 68, 0.15)\",\n",
    "#             fill=fill,\n",
    "#         )\n",
    "#         data.append(main_line)\n",
    "#\n",
    "#     if show_error:\n",
    "#         upper_bound = go.Scatter(\n",
    "#             name=\"Upper Bound\",\n",
    "#             x=new_x_vals,\n",
    "#             y=new_y_hi,\n",
    "#             mode=\"lines\",\n",
    "#             marker=dict(color=\"#444\"),\n",
    "#             line=dict(width=0, shape=\"hv\"),\n",
    "#             fillcolor=\"rgba(68, 68, 68, 0.15)\",\n",
    "#             fill=\"tonexty\",\n",
    "#         )\n",
    "#         lower_bound = go.Scatter(\n",
    "#             name=\"Lower Bound\",\n",
    "#             x=new_x_vals,\n",
    "#             y=new_y_lo,\n",
    "#             marker=dict(color=\"#444\"),\n",
    "#             line=dict(width=0, shape=\"hv\"),\n",
    "#             mode=\"lines\",\n",
    "#         )\n",
    "#         data = [lower_bound, main_line, upper_bound]\n",
    "#\n",
    "#     show_legend = True if multiclass or not show_error else False\n",
    "#     layout = go.Layout(\n",
    "#         title=title,\n",
    "#         showlegend=show_legend,\n",
    "#         xaxis=dict(title=xtitle),\n",
    "#         yaxis=dict(title=ytitle),\n",
    "#     )\n",
    "#     yrange = None\n",
    "#     if data_dict.get(\"scores_range\", None) is not None:\n",
    "#         scores_range = data_dict[\"scores_range\"]\n",
    "#         yrange = scores_range\n",
    "#\n",
    "#     main_fig = go.Figure(data=data, layout=layout)\n",
    "#     main_fig.show()\n",
    "#     main_fig.write_image(f'plots/{model_name}_{dataset_name}_shape_{feature_name}.pdf')\n",
    "\n",
    "\n",
    "def make_plot_ebm(data_dict, feature_name, model_name, dataset_name, num_epochs='', debug=False):\n",
    "    x_vals = data_dict[\"names\"].copy()\n",
    "    y_vals = data_dict[\"scores\"].copy()\n",
    "\n",
    "    # This is important since you do not plot plt.stairs with len(edges) == len(vals) + 1, which will have a drop to zero at the end\n",
    "    y_vals = np.r_[y_vals, y_vals[np.newaxis, -1]]\n",
    "\n",
    "    # This is the code interpretml also uses: https://github.com/interpretml/interpret/blob/2327384678bd365b2c22e014f8591e6ea656263a/python/interpret-core/interpret/visual/plot.py#L115\n",
    "\n",
    "    # main_line = go.Scatter(\n",
    "    #     x=x_vals,\n",
    "    #     y=y_vals,\n",
    "    #     name=\"Main\",\n",
    "    #     mode=\"lines\",\n",
    "    #     line=dict(color=\"rgb(31, 119, 180)\", shape=\"hv\"),\n",
    "    #     fillcolor=\"rgba(68, 68, 68, 0.15)\",\n",
    "    #     fill=\"none\",\n",
    "    # )\n",
    "    #\n",
    "    # main_fig = go.Figure(data=[main_line])\n",
    "    # main_fig.show()\n",
    "    # main_fig.write_image(f'plots/{model_name}_{dataset_name}_shape_{feature_name}_{num_epochs}epochs.pdf')\n",
    "\n",
    "\n",
    "    # This is my custom code used for plotting\n",
    "    x = np.array(x_vals)\n",
    "    if debug:\n",
    "        print(\"Num cols:\", num_cols)\n",
    "    if feature_name in num_cols:\n",
    "        if debug:\n",
    "            print(\"Feature to scale back:\", feature_name)\n",
    "        x = scaler_dict[feature_name].inverse_transform(x.reshape(-1, 1)).squeeze()\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"Feature not to scale back:\", feature_name)\n",
    "\n",
    "    plt.step(x, y_vals, where=\"post\", color='black')\n",
    "    # plt.fill_between(x, lower_bounds, mean, color='gray')\n",
    "    # plt.fill_between(x, mean, upper_bounds, color='gray')\n",
    "    plt.xlabel(f'Feature value')\n",
    "    plt.ylabel('Feature effect on model output')\n",
    "    plt.title(f'Feature:{feature_name}')\n",
    "    plt.savefig(f'plots/{model_name}_{dataset_name}_shape_{feature_name}_{num_epochs}epochs.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def make_plot_interaction(left_names, right_names, scores, feature_name_left, feature_name_right, model_name,\n",
    "                          dataset_name):\n",
    "    left_names = np.array(left_names)\n",
    "    # print(right_names)\n",
    "    if feature_name_left in num_cols:\n",
    "        left_names = scaler_dict[feature_name_left].inverse_transform(left_names.reshape(-1, 1)).squeeze()\n",
    "    right_names = np.array(right_names)\n",
    "    if feature_name_right in num_cols:\n",
    "        right_names = scaler_dict[feature_name_right].inverse_transform(right_names.reshape(-1, 1)).squeeze()\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.pcolormesh(left_names, right_names, scores, shading='auto')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.xlabel(feature_name_left)\n",
    "    plt.ylabel(feature_name_right)\n",
    "    plt.savefig(f'plots/{model_name}_{dataset_name}_interact_{feature_name_left.replace(\"?\", \"\")} x {feature_name_right.replace(\"?\", \"\")}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_one_hot_plot(class_zero, class_one, feature_name, model_name, dataset_name):\n",
    "    plt.bar([0, 1], [class_zero, class_one], color='gray', tick_label=[f'{feature_name} = 0', f'{feature_name} = 1'])\n",
    "    plt.ylabel('Feature effect on model output')\n",
    "    plt.title(f'Feature:{feature_name}')\n",
    "    print(feature_name)\n",
    "    plt.savefig(f'plots/{model_name}_{dataset_name}_onehot_{str(feature_name).replace(\"?\", \"\")}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def EBM_show(X, y):\n",
    "    m4 = ExplainableBoostingRegressor(interactions=40, max_bins=256)\n",
    "    m4.fit(X, y)\n",
    "    ebm_global = m4.explain_global()\n",
    "    show(ebm_global)\n",
    "\n",
    "\n",
    "def EBM(X, y, dataset_name, model_name='EBM'):\n",
    "    if task == \"classification\":\n",
    "        ebm= ExplainableBoostingClassifier(interactions=50, max_bins=256)\n",
    "    else:\n",
    "        ebm = ExplainableBoostingRegressor(interactions=50, max_bins=256)\n",
    "    ebm.fit(X, y)\n",
    "    ebm_global = ebm.explain_global()\n",
    "\n",
    "    for i in range(len(ebm_global.data()['names'])):\n",
    "        data_names = ebm_global.data()\n",
    "        print(data_names['names'][i])\n",
    "        feature_name = data_names['names'][i]\n",
    "        shape_data = ebm_global.data(i)\n",
    "\n",
    "        if shape_data['type'] == 'interaction':\n",
    "            x_name, y_name = feature_name.split(' x ')\n",
    "            x_name = x_name.replace(' ', '')\n",
    "            y_name = y_name.replace(' ', '')\n",
    "            make_plot_interaction(shape_data['left_names'], shape_data['right_names'],\n",
    "                                  np.transpose(shape_data['scores']),\n",
    "                                  x_name, y_name, model_name, dataset_name)\n",
    "            continue\n",
    "        if len(shape_data['names']) == 2:\n",
    "            make_one_hot_plot(shape_data['scores'][0], shape_data['scores'][1], feature_name, model_name, dataset_name)\n",
    "        else:\n",
    "            make_plot_ebm(shape_data, feature_name, model_name, dataset_name)\n",
    "            # plot_continuous_bar(shape_data, feature_name, model_name, dataset_name)\n",
    "\n",
    "    feat_for_vis = dict()\n",
    "    for i, n in enumerate(ebm_global.data()['names']):\n",
    "        feat_for_vis[n] = {'importance': ebm_global.data()['scores'][i]}\n",
    "    feature_importance_visualize(feat_for_vis, save_png=True, folder='.', name='ebm_feat_imp')\n",
    "\n",
    "\n",
    "# def modify_interaction_ranges(ebm_global, min_heatmap_val, max_heatmap_val):\n",
    "#     for data_dict in ebm_global._internal_obj['specific']:\n",
    "#         if data_dict['type'] == 'interaction':\n",
    "#             data_dict['scores_range'] = (min_heatmap_val, max_heatmap_val)\n",
    "\n",
    "\n",
    "\n",
    "def GAM(X, y, dataset_name, model_name='GAM'):\n",
    "    if task == \"classification\":\n",
    "        gam = LogisticGAM().fit(X, y)\n",
    "    elif task == \"regression\":\n",
    "        gam = LinearGAM().fit(X, y)\n",
    "    for i, term in enumerate(gam.terms):\n",
    "        if term.isintercept:\n",
    "            continue\n",
    "        XX = gam.generate_X_grid(term=i)\n",
    "        pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n",
    "        # make_plot(XX[:,i].squeeze(), pdep, confi[:,0], confi[:,1], X.columns[i])\n",
    "        if len(X[X.columns[i]].unique()) == 2:\n",
    "            make_one_hot_plot(pdep[0], pdep[-1], X.columns[i], model_name, dataset_name)\n",
    "        else:\n",
    "            make_plot(XX[:, i].squeeze(), pdep, pdep, pdep, X.columns[i], model_name, dataset_name)\n",
    "\n",
    "def Gaminet(X, y, dataset_name, model_name='Gaminet'):\n",
    "    meta_info = {X.columns[i]: {'type': 'continuous'} for i in range(len(X.columns))}\n",
    "    meta_info.update({'Y': {'type': 'target'}})\n",
    "\n",
    "    # from sklearn.preprocessing import FunctionTransformer\n",
    "    identity = FunctionTransformer()\n",
    "\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == 'target':\n",
    "            continue\n",
    "        # sx = MinMaxScaler((0, 1))\n",
    "        # sx.fit([[0], [1]])\n",
    "        # print(scaler_dict.keys())\n",
    "        # print(X.columns)\n",
    "        if key in scaler_dict:\n",
    "            meta_info[key]['scaler'] = scaler_dict[key]\n",
    "        else:\n",
    "            meta_info[key]['scaler'] = identity\n",
    "\n",
    "    if task == \"classification\":\n",
    "        model_to_run = GAMINet(meta_info=meta_info, interact_num=20,\n",
    "                               interact_arch=[40] * 5, subnet_arch=[40] * 5,\n",
    "                               batch_size=1024, task_type=\"Classification\", activation_func=tf.nn.relu,\n",
    "                               main_effect_epochs=1000, interaction_epochs=1000, tuning_epochs=200,\n",
    "                               # todo: main_effect_epochs=5000, interaction_epochs=5000, tuning_epochs=500,\n",
    "                               lr_bp=[0.0001, 0.0001, 0.0001], early_stop_thres=[50, 50, 50],\n",
    "                               heredity=True, loss_threshold=0.01, reg_clarity=1,\n",
    "                               mono_increasing_list=[], mono_decreasing_list=[],  # the indices list of features\n",
    "                               verbose=True, val_ratio=0.2, random_state=random_state)\n",
    "        print(np.array(y).shape)\n",
    "        model_to_run.fit(np.array(X), np.array(y).reshape(-1, 1))\n",
    "\n",
    "    elif task == \"regression\":\n",
    "        model_to_run = GAMINet(meta_info=meta_info, interact_num=20,\n",
    "                               interact_arch=[40] * 5, subnet_arch=[40] * 5,\n",
    "                               batch_size=1024, task_type=\"Regression\", activation_func=tf.nn.relu,\n",
    "                               main_effect_epochs=5000, interaction_epochs=5000, tuning_epochs=500,\n",
    "                               lr_bp=[0.0001, 0.0001, 0.0001], early_stop_thres=[50, 50, 50],\n",
    "                               heredity=True, loss_threshold=0.01, reg_clarity=1,\n",
    "                               mono_increasing_list=[], mono_decreasing_list=[],  # the indices list of features\n",
    "                               verbose=True, val_ratio=0.2, random_state=random_state)\n",
    "        model_to_run.fit(np.array(X), np.array(y))\n",
    "\n",
    "    data_dict = model_to_run.global_explain(save_dict=False, main_grid_size=1000)\n",
    "\n",
    "    Xnames2Featurenames = dict(zip([X.columns[i] for i in range(X.shape[1])], X.columns))\n",
    "    print(Xnames2Featurenames)\n",
    "\n",
    "    for k in data_dict.keys():\n",
    "        if data_dict[k]['type'] == 'pairwise':\n",
    "            feature_name_left, feature_name_right = k.split('vs. ')\n",
    "            feature_name_left = feature_name_left.replace(' ', '')\n",
    "            feature_name_right = feature_name_right.replace(' ', '')\n",
    "            make_plot_interaction(data_dict[k]['input1'], data_dict[k]['input2'], data_dict[k]['outputs'],\n",
    "                                  Xnames2Featurenames[feature_name_left],\n",
    "                                  Xnames2Featurenames[feature_name_right], model_name, dataset_name)\n",
    "        elif data_dict[k]['type'] == 'continuous':\n",
    "            # todo: reverse:\n",
    "            # if len(X[Xnames2Featurenames[k]].unique()) == 2:\n",
    "            #     make_one_hot_plot(data_dict[k]['outputs'][0], data_dict[k]['outputs'][-1],\n",
    "            #                       Xnames2Featurenames[k], model_name, dataset_name)\n",
    "            # else:\n",
    "            try:\n",
    "                make_plot(data_dict[k]['inputs'], data_dict[k]['outputs'], data_dict[k]['outputs'],\n",
    "                          data_dict[k]['outputs'], Xnames2Featurenames[k], model_name, dataset_name)\n",
    "            except:\n",
    "                print(\"not continous\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    feat_for_vis = dict()\n",
    "    for i, k in enumerate(data_dict.keys()):\n",
    "        if 'vs.' in k:\n",
    "            feature_name_left, feature_name_right = k.split('vs. ')\n",
    "            feature_name_left = feature_name_left.replace(' ', '')\n",
    "            feature_name_right = feature_name_right.replace(' ', '')\n",
    "            feature_name_left = Xnames2Featurenames[feature_name_left]\n",
    "            feature_name_right = Xnames2Featurenames[feature_name_right]\n",
    "            feat_for_vis[f'{feature_name_left}\\nvs.\\n{feature_name_right}'] = {'importance': data_dict[k]['importance']}\n",
    "        else:\n",
    "            feat_for_vis[Xnames2Featurenames[k]] = {'importance': data_dict[k]['importance']}\n",
    "\n",
    "    feature_importance_visualize(feat_for_vis, save_png=True, folder='.', name='gaminet_feat_imp')\n",
    "\n",
    "def LR(X, y, dataset_name, model_name='LR'):\n",
    "    m = Ridge()\n",
    "    # if task == 'regression':\n",
    "    # else:\n",
    "    #     m = LogisticRegression()\n",
    "    m.fit(X, y)\n",
    "    import seaborn as sns\n",
    "    #plot = sns.distplot(m.coef_)\n",
    "    word_to_coef = dict(zip(m.feature_names_in_, m.coef_.squeeze()))\n",
    "    dict(sorted(word_to_coef.items(), key=lambda item: item[1]))\n",
    "    word_to_coef_df = pd.DataFrame.from_dict(word_to_coef, orient='index')\n",
    "    print(word_to_coef_df)\n",
    "\n",
    "    for i, feature_name in enumerate(X.columns):\n",
    "        inp = torch.linspace(X[feature_name].min(), X[feature_name].max(), 1000)\n",
    "        outp = word_to_coef[feature_name] *  inp #+ m.intercept_\n",
    "        # outp = nam_model.feature_nns[i](inp).detach().numpy().squeeze()\n",
    "        # if len(X[feature_name].unique()) == 2:\n",
    "        #     make_one_hot_plot(outp[0], outp[-1], feature_name, model_name, dataset_name)\n",
    "        # else:\n",
    "        make_plot(inp, outp, outp, outp, feature_name, model_name, dataset_name)\n",
    "\n",
    "# EBM_show(X, y) # for EBM_Show copy paste this script into a jupyter notebook and only run the EBM_Show dashboard\n",
    "# EBM(X, y, dataset_name)\n",
    "# GAM(X, y, dataset_name)\n",
    "# Gaminet(X, y, dataset_name)\n",
    "# LR(X, y, dataset_name)\n",
    "# X.to_csv(f'export/X_full_{dataset_name}.csv')\n",
    "# pd.DataFrame(y).to_csv(f'export/y_full_{dataset_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/1783941160432/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/1783941160432/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EBM_show(X, y) # for EBM_Show copy paste this script into a jupyter notebook and only run the EBM_Show dashboard\n",
    "# EBM(X, y, dataset_name)\n",
    "# GAM(X, y, dataset_name)\n",
    "# Gaminet(X, y, dataset_name)\n",
    "# LR(X, y, dataset_name)\n",
    "# X.to_csv(f'export/X_full_{dataset_name}.csv')\n",
    "# pd.DataFrame(y).to_csv(f'export/y_full_{dataset_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}